{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# define a transform to normalize the data\n",
    "# if the img has three channels, you should have three number for mean, \n",
    "# for example, img is RGB, mean is [0.5, 0.5, 0.5], the normalize result is R * 0.5, G * 0.5, B * 0.5. \n",
    "# If img is grey type that only one channel, mean should be [0.5], the normalize result is R * 0.5\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])\n",
    "                               ])\n",
    "# download and load the traning data\n",
    "trainset = datasets.MNIST('data/MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([1, 28, 28])\n",
      "torch.Size([64, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# make an iterator for looping\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images[0].shape)\n",
    "# NOTE: The batch size is the number of images we get in one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(images[1].numpy().squeeze(), cmap='Greys_r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to create a dense fully-connected network. \n",
    "\n",
    "Each unit in one layer is connected to the other in the next layer.\n",
    "The input to each layer must be one-dimensional vector. But our images are 28*28 2D tensors, so we need to convert them to 1D vectors. Therefore:\n",
    "* Convert/Flatten the batch of images of shape(64, 1, 28, 28) into (64, 28 * 28=784).\n",
    "* For the output layer, we also need 10 output units for the 10 classes(digits)\n",
    "* Also convert the network output into a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_images = images.view(64, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 784])\n"
     ]
    }
   ],
   "source": [
    "print(flattened_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\"Create a sigmoid activation function.\n",
    "    Good for outputs that fall between 0 and 1. (probability)\n",
    "    args x: a torch tensor.\n",
    "    \"\"\"\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Create a softmax activation function.\n",
    "    Good for outputs that fall between 0 and 1. (probability)\n",
    "    args x: a torch tensor.\n",
    "    \"\"\"\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "# flatten the images to shape(64, 784)\n",
    "inputs = images.view(images.shape[0], -1)\n",
    "\n",
    "# create parameters\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inputs, w1) + b1)\n",
    "\n",
    "out = torch.mm(h, w2) + b2\n",
    "probabilities = softmax(out)\n",
    "print(probabilities.shape)\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Torch nn to create networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Network(nn.Module):\n",
    "    \"\"\"Use relu(Rectified linear unit) as the activation function.\n",
    "    Networks tend to train a lot faster when using relu.\n",
    "    For a network to approximate a non-linear function, the activation\n",
    "    function must be non-linear.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # inputs to hidden layer linear transformation\n",
    "        self.hidden_layer1 = nn.Linear(784, 128) # 256 outputs\n",
    "        self.hidden_layer2 = nn.Linear(128, 64)\n",
    "        # output layer, 10 units one for each digit\n",
    "        self.output = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # hidden layer with sigmoid activation\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "        x = F.relu(self.hidden_layer2(x))\n",
    "        # Output layer with softmax activation\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden_layer1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (hidden_layer2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (output): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3058, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10)\n",
    "                     )\n",
    "\n",
    "# define the loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Prepare data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "# flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# forward pass, get the logits\n",
    "logits = model(images)\n",
    "# calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's more convenient to build model with a log-softmax output using `nn.LogSoftmax`\n",
    "We can get actual probabilities by taking the exponential torch.exp(output).\n",
    "We'll also use the negative log likelihood loss, `nn.NLLLoss`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3025, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1),\n",
    "                     )\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "logits = model(images)\n",
    "loss = criterion(logits, labels)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USing Autograd to perform backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating loss, we perform backpropagation. Enter `autograd`\n",
    "\n",
    "We use it to calculate the gradients of all our parameters with respect to the loss we got. Autograd goes backwards through the tensor operations, calculating gradients along the way. \n",
    "* Set `requires_grad=True` on a tensor when creating the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4997, -0.1425],\n",
      "        [-0.8944,  0.0633]])\n",
      "tensor([[-0.4997, -0.1425],\n",
      "        [-0.8944,  0.0633]], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2,2, requires_grad=True)\n",
    "y = x** 2\n",
    "z = y.mean()\n",
    "z.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back to the model we created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[-0.0029, -0.0029, -0.0029,  ..., -0.0029, -0.0029, -0.0029],\n",
      "        [-0.0028, -0.0028, -0.0028,  ..., -0.0028, -0.0028, -0.0028],\n",
      "        [-0.0006, -0.0006, -0.0006,  ..., -0.0006, -0.0006, -0.0006],\n",
      "        ...,\n",
      "        [-0.0011, -0.0011, -0.0011,  ..., -0.0011, -0.0011, -0.0011],\n",
      "        [-0.0036, -0.0036, -0.0036,  ..., -0.0036, -0.0036, -0.0036],\n",
      "        [-0.0026, -0.0026, -0.0026,  ..., -0.0026, -0.0026, -0.0026]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need an optimizer that'll update weights with the gradients from the backward pass.\n",
    "From Pytorch's `optim` package, we can use stochastic gradient descenc with `optim.SGD`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "# pass in the parameter to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.0614541531689385\n",
      "Training loss: 0.3895804712147728\n",
      "Training loss: 0.3246205799631091\n",
      "Training loss: 0.29300587304206543\n",
      "Training loss: 0.26864237868900237\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1),\n",
    "                     )\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten Images into 784 long vector for the input layer\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        \n",
    "        # clear gradients because they accumulate\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass\n",
    "        output = model.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f'Training loss: {running_loss/len(trainloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a helper to view the probability distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def view_classify(img, ps):\n",
    "    \"\"\"Function for viewing an image and it's predicted classes.\"\"\"\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.2855e-02, 5.0043e-05, 7.8326e-04, 5.7256e-03, 4.5138e-03, 9.6925e-01,\n",
      "         1.3272e-03, 1.2127e-03, 5.4744e-04, 3.7324e-03]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "img = images[0].view(1, 784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "    \n",
    "    ps = F.softmax(logits, dim=1)\n",
    "    print(ps)\n",
    "    view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
